# Web Crawling Multi-Livello

## Panoramica

La funzionalitÃ  di **Crawling Intelligente Multi-Livello** permette di estrarre automaticamente contenuti da piÃ¹ pagine di un sito web, seguendo i link interni fino a una profonditÃ  specificata.

## Come Funziona

### 1. Input dell'Utente
L'utente inserisce l'URL homepage del sito e attiva il crawling con questi parametri:
- **ProfonditÃ ** (1-3 livelli): quanti "click" di distanza dalla homepage esplorare
- **Max Pagine** (5-50): numero massimo di pagine da analizzare

### 2. Processo di Crawling

#### Fase 1: Estrazione Link
- Analizza la homepage e estrae tutti i link interni (stesso dominio)
- Filtra automaticamente:
  - File binari (PDF, immagini, ZIP, ecc.)
  - Pagine di login/logout
  - Pagine amministrative
  - Cart/checkout
  - Link con solo hash (#)

#### Fase 2: Navigazione Multi-Livello
```
Livello 0: Homepage
    â”œâ”€ Livello 1: Link dalla homepage
    â”‚   â”œâ”€ Livello 2: Link dalle pagine di livello 1
    â”‚   â””â”€ ...
    â””â”€ ...
```

#### Fase 3: Estrazione Contenuto
Per ogni pagina visitata:
- Estrae il contenuto testuale principale
- Rimuove navigazione, footer, sidebar, ads
- Conta le parole
- Salva metadati (titolo, URL, descrizione)

#### Fase 4: Aggregazione Intelligente
- **Ordinamento per importanza**: le pagine con piÃ¹ contenuto sono considerate piÃ¹ importanti
- **Limite intelligente**: combina fino a 20.000 parole totali
- **Minimo garantito**: include sempre almeno le prime 3 pagine anche se superano il limite
- **Struttura chiara**: separa le pagine con intestazioni e divisori

### 3. Output Finale

Il contenuto aggregato viene formattato cosÃ¬:
```
=== Pagina 1: Chi siamo ===
URL: https://example.com/about
Parole: 856

[contenuto della pagina]

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

=== Pagina 2: Storia ===
URL: https://example.com/storia
Parole: 642

[contenuto della pagina]
```

## Vantaggi

### vs. Solo Homepage
- âœ… **Contenuto 10-20x piÃ¹ ricco**: da 600 a 6000+ parole
- âœ… **Contesto completo**: storia, valori, servizi, team
- âœ… **Style guide piÃ¹ accurato**: analizza piÃ¹ esempi di scrittura
- âœ… **Metadati piÃ¹ completi**: piÃ¹ informazioni sull'azienda/autore

### vs. Multi-URL Manuale
- âœ… **Automatico**: non serve specificare ogni URL
- âœ… **Scopre pagine nascoste**: trova pagine non linkate nel menu principale
- âœ… **PiÃ¹ veloce**: non serve cercare manualmente gli URL giusti

## Limiti di Sicurezza

### Rispetto per i Server
- **Delay tra richieste**: 500ms di pausa tra ogni pagina
- **User-Agent identificabile**: si presenta come un bot legittimo
- **Timeout**: 15 secondi max per pagina
- **Max pagine hard-coded**: 50 pagine massimo (anche se l'utente chiede di piÃ¹)

### Filtri Anti-Spam
- Esclude pagine duplicate (stesso URL)
- Esclude pagine senza contenuto (<100 caratteri)
- Esclude errori HTTP
- Esclude pagine non-HTML

### Gestione Errori
- Continua anche se alcune pagine falliscono
- Registra gli errori nel log ma non blocca il processo
- Restituisce errore solo se TUTTE le pagine falliscono

## Configurazione Consigliata

### Per Siti Piccoli (5-20 pagine)
- **ProfonditÃ **: 3 livelli
- **Max Pagine**: 20
- **Tempo stimato**: 15-30 secondi

### Per Siti Medi (20-100 pagine)
- **ProfonditÃ **: 2 livelli (consigliato)
- **Max Pagine**: 30
- **Tempo stimato**: 20-40 secondi

### Per Siti Grandi (100+ pagine)
- **ProfonditÃ **: 2 livelli
- **Max Pagine**: 50
- **Tempo stimato**: 30-60 secondi

## Esempio di Log Console

```
ðŸ•·ï¸ Starting crawl: https://www.youcanprint.it (depth: 2, max pages: 20)
ðŸ“„ Crawling [0]: https://www.youcanprint.it
ðŸ”— Found 28 internal links at depth 0
ðŸ“„ Crawling [1]: https://www.youcanprint.it/chi-siamo
ðŸ“„ Crawling [1]: https://www.youcanprint.it/servizi
ðŸ“„ Crawling [1]: https://www.youcanprint.it/distribuzione
ðŸ”— Found 15 internal links at depth 1
ðŸ“„ Crawling [2]: https://www.youcanprint.it/servizi/editing
ðŸ“„ Crawling [2]: https://www.youcanprint.it/servizi/copertina
âš ï¸ Skipping https://www.youcanprint.it/login: Excluded pattern
ðŸ“Š Selected 12/15 pages for analysis (18456 words)
âœ… Crawl complete: 15 pages crawled, 12 pages included, 18456 total words
```

## API Endpoint

### Request
```typescript
POST /api/projects/analyze-website
{
  "url": "https://example.com",
  "crawl": true,           // Abilita crawling
  "maxDepth": 2,           // ProfonditÃ  (default: 2)
  "maxPages": 20           // Max pagine (default: 20)
}
```

### Response
```typescript
{
  "success": true,
  "projectData": { ... },
  "styleGuide": "...",
  "extractionInfo": {
    "url": "https://example.com",
    "wordCount": 18456,
    "title": "...",
    // ...
  }
}
```

## Testing

### Test Manuale
1. Apri l'app e clicca "Crea Progetto da Sito Web"
2. Inserisci: `https://www.youcanprint.it`
3. Attiva "ðŸ•·ï¸ Crawling Intelligente Multi-Livello"
4. Imposta profonditÃ : 2, max pagine: 20
5. Clicca "Analizza Sito"
6. Attendi 30-60 secondi
7. Verifica che il form sia popolato con dati molto piÃ¹ ricchi

### Confronto
Prova lo stesso URL con e senza crawling per vedere la differenza:
- **Senza crawling**: ~600-800 parole dalla homepage
- **Con crawling**: ~5000-15000 parole da 10-20 pagine

## Prossimi Miglioramenti

- [ ] Sitemap.xml parsing per link prioritari
- [ ] Intelligenza per identificare pagine "importanti" (About, Team, Storia)
- [ ] Cache dei risultati per evitare di crawlare lo stesso sito piÃ¹ volte
- [ ] Supporto per robots.txt
- [ ] Rate limiting piÃ¹ sofisticato per siti grandi
- [ ] Estrazione di immagini e metadata SEO

## Troubleshooting

### "Timeout: il sito non risponde"
- Il sito Ã¨ lento o ha protezione anti-bot
- Soluzione: riduci max pagine o profonditÃ 

### "Contenuto troppo breve"
- Il sito ha poco testo o usa molto JavaScript
- Soluzione: usa multi-URL manuale invece del crawling

### Ci mette troppo tempo
- Troppi livelli o troppe pagine
- Soluzione: riduci a profonditÃ  1-2 e max 10-15 pagine

### Non trova pagine importanti
- Le pagine sono linkate in modo complesso
- Soluzione: usa multi-URL manuale per specificare pagine precise
